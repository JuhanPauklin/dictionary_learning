{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639db6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning.evaluation import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning import ActivationBuffer, AutoEncoder\n",
    "from dictionary_learning.trainers import StandardTrainer\n",
    "from dictionary_learning.training import trainSAE\n",
    "from dictionary_learning.utils import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f914de85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/helios/home/jpauklin/dictionary_learning/dictionary_learning/dictionary.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = t.load(path)\n"
     ]
    }
   ],
   "source": [
    "ae = AutoEncoder.from_pretrained(\"/gpfs/helios/home/jpauklin/dictionary_learning/saes/estMedSae120425/trainer_0/ae_12042025.pt\").to(\"cuda\") # to is rquired to load to GPU\n",
    "\n",
    "data = read_csv(\"/gpfs/space/projects/stacc_health/data-synthetic/100k_synthetic_texts.csv\", 100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba905ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" #GPU\n",
    "# Load Model\n",
    "model_name = \"/gpfs/space/projects/stacc_health/gpt2_model/estMed-gpt2_fine_tuned4/estMed-gpt2_fine_tuned4\"\n",
    "model = LanguageModel(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "activation_dim = model.transformer.h[0].ln_1.normalized_shape[0] # output dimension of the MLP = 768\n",
    "submodule = model.transformer.h[11].mlp\n",
    "\n",
    "buffer = ActivationBuffer( # buffer will yield batches of tensors of dimension = submodule's output dimension\n",
    "    data=data,\n",
    "    model=model,\n",
    "    submodule=submodule,\n",
    "    d_submodule=activation_dim, # output dimension of the model component\n",
    "    n_ctxs=3e4,  # length of each context. you can set this higher or lower dependong on your available memory\n",
    "    device=device,\n",
    "    out_batch_size = 2048 # reduce batch size to limit memory usage.\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61bfafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = evaluate(dictionary = ae, \n",
    "               activations = buffer,\n",
    "               device = device,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490738f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    MSE loss: average squared L2 distance between an activation and the autoencoder's reconstruction of it\n",
    "    L1 loss: a measure of the autoencoder's sparsity\n",
    "    L0: average number of features active above a random token\n",
    "    Percentage of neurons alive: fraction of the dictionary features which are active on at least one token out of dictionary.dict_size random tokens\n",
    "    \n",
    "    CE diff: difference between the usual cross-entropy loss of the model for next token prediction and the cross entropy when replacing activations with our dictionary's reconstruction\n",
    "    Percentage of CE loss recovered: when replacing the activation with the dictionary's reconstruction, the percentage of the model's cross-entropy loss on next token prediction that is recovered (relative to the baseline of zero ablating the activation)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2572080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2_loss': 1.833837628364563,\n",
       " 'l1_loss': 101.27059173583984,\n",
       " 'l0': 1205.56884765625,\n",
       " 'frac_variance_explained': 0.9800195693969727,\n",
       " 'cossim': 0.9891963601112366,\n",
       " 'l2_ratio': 0.9396045207977295,\n",
       " 'relative_reconstruction_bias': 0.9661658406257629,\n",
       " 'loss_original': 4.567016124725342,\n",
       " 'loss_reconstructed': 4.566261291503906,\n",
       " 'loss_zero': 4.4883832931518555,\n",
       " 'frac_recovered': 0.9904005527496338,\n",
       " 'frac_alive': 0.9456380605697632}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
