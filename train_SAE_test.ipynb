{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8598325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 50_000_000 # numbrid p√§rit dictionary_learning_demo/demo_config.py-st\n",
    "sae_batch_size = 2048\n",
    "steps = int(num_tokens / sae_batch_size) # Total number of batches to train\n",
    "log_steps = 100  # Log the training on wandb or print to console every log_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fec79b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trainer': <class 'dictionary_learning.trainers.standard.StandardTrainer'>, 'dict_class': <class 'dictionary_learning.dictionary.AutoEncoder'>, 'activation_dim': 512, 'dict_size': 8192, 'lr': 0.001, 'device': 'cuda:0'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "StandardTrainer.__init__() missing 3 required positional arguments: 'steps', 'layer', and 'lm_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m\n\u001b[1;32m     34\u001b[0m trainer_cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m: StandardTrainer,\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_class\u001b[39m\u001b[38;5;124m\"\u001b[39m: AutoEncoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device,\n\u001b[1;32m     41\u001b[0m }\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# train the sparse autoencoder (SAE)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m ae \u001b[38;5;241m=\u001b[39m \u001b[43mtrainSAE\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# you could also use another (i.e. pytorch dataloader) here instead of buffer\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtrainer_cfg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary_learning/dictionary_learning/training.py:150\u001b[0m, in \u001b[0;36mtrainSAE\u001b[0;34m(data, trainer_configs, steps, use_wandb, wandb_entity, wandb_project, save_steps, save_dir, log_steps, activations_split_by_head, transcoder, run_cfg, normalize_activations, verbose, device, autocast_dtype)\u001b[0m\n\u001b[1;32m    148\u001b[0m     trainer_class \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 150\u001b[0m     trainers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrainer_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    152\u001b[0m wandb_processes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    153\u001b[0m log_queues \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: StandardTrainer.__init__() missing 3 required positional arguments: 'steps', 'layer', and 'lm_name'"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from dictionary_learning import ActivationBuffer, AutoEncoder\n",
    "from dictionary_learning.trainers import StandardTrainer\n",
    "from dictionary_learning.training import trainSAE\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\" # can be any Huggingface model\n",
    "\n",
    "model = LanguageModel(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    ")\n",
    "submodule = model.gpt_neox.layers[1].mlp # layer 1 MLP\n",
    "activation_dim = 512 # output dimension of the MLP\n",
    "dictionary_size = 16 * activation_dim\n",
    "\n",
    "# data must be an iterator that outputs strings\n",
    "data = iter(\n",
    "    [\n",
    "        \"This is some example data\",\n",
    "        \"In real life, for training a dictionary\",\n",
    "        \"you would need much more data than this\",\n",
    "    ]\n",
    ")\n",
    "buffer = ActivationBuffer(\n",
    "    data=data,\n",
    "    model=model,\n",
    "    submodule=submodule,\n",
    "    d_submodule=activation_dim, # output dimension of the model component\n",
    "    n_ctxs=3e4,  # you can set this higher or lower dependong on your available memory\n",
    "    device=device,\n",
    ")  # buffer will yield batches of tensors of dimension = submodule's output dimension\n",
    "\n",
    "trainer_cfg = {\n",
    "    \"trainer\": StandardTrainer,\n",
    "    \"dict_class\": AutoEncoder,\n",
    "    \"activation_dim\": activation_dim,\n",
    "    \"dict_size\": dictionary_size,\n",
    "    \"lr\": 1e-3,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "# train the sparse autoencoder (SAE)\n",
    "ae = trainSAE(\n",
    "    data=buffer,  # you could also use another (i.e. pytorch dataloader) here instead of buffer\n",
    "    trainer_configs=[trainer_cfg],\n",
    "    steps=steps,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
